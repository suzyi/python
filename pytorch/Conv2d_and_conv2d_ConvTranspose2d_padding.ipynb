{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples of `torch.nn.Conv2d`, `torch.nn.functional.conv2d` and `torch.nn.ConvTranspose2d`\n",
    "\n",
    "Guorui Shen, guorui233@outlook.com, Jul 28, 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn.Conv2d`\n",
    "+ `CLASS torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')`\n",
    "\n",
    "```\n",
    "conv = torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n",
    "y = conv(x) # here x is of shape (batch, channel, height, width)\n",
    "\n",
    "```\n",
    "+ padding_mode (string, optional) – 'zeros', 'reflect', 'replicate' or 'circular'. Default: 'zeros'\n",
    "+ https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------input------------------\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "------------------output------------------\n",
      "tensor([[[[1.4973, 2.2425],\n",
      "          [3.7328, 4.4779]]]], grad_fn=<MkldnnConvolutionBackward>)\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(\"------------------input------------------\")\n",
    "im = torch.arange(9, dtype=torch.float).reshape((1, 1, 3, 3))\n",
    "print(im)\n",
    "print(im.shape)\n",
    "print(\"------------------output------------------\")\n",
    "win_size = 2\n",
    "res = torch.nn.Conv2d(in_channels=1, out_channels=1, kernel_size=win_size, padding=0, padding_mode='zeros')(im)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn.functional.conv2d`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------input------------------\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "------------------weights------------------\n",
      "tensor([[[[0.2500, 0.2500],\n",
      "          [0.2500, 0.2500]]]])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "------------------output------------------\n",
      "tensor([[[[2., 3.],\n",
      "          [5., 6.]]]])\n",
      "torch.Size([1, 1, 2, 2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(\"------------------input------------------\")\n",
    "im = torch.arange(9, dtype=torch.float).reshape((1, 1, 3, 3))\n",
    "print(im)\n",
    "print(im.shape)\n",
    "print(\"------------------weights------------------\")\n",
    "win_size = 2\n",
    "weights = torch.ones((win_size, win_size), dtype=torch.float)\n",
    "weights = weights/sum(sum(weights))\n",
    "weights= weights.reshape((1, 1, win_size, win_size))\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(\"------------------output------------------\")\n",
    "res = F.conv2d(im, weight=weights, stride=1, padding=0)\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `torch.nn.functional.conv2d` and padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------input------------------\n",
      "tensor([[[[0., 1., 2.],\n",
      "          [3., 4., 5.],\n",
      "          [6., 7., 8.]]]])\n",
      "torch.Size([1, 1, 3, 3])\n",
      "------------------padded input------------------\n",
      "tensor([[[[0., 1., 2., 0., 1., 2., 0., 1., 2.],\n",
      "          [3., 4., 5., 3., 4., 5., 3., 4., 5.],\n",
      "          [6., 7., 8., 6., 7., 8., 6., 7., 8.],\n",
      "          [0., 1., 2., 0., 1., 2., 0., 1., 2.],\n",
      "          [3., 4., 5., 3., 4., 5., 3., 4., 5.],\n",
      "          [6., 7., 8., 6., 7., 8., 6., 7., 8.],\n",
      "          [0., 1., 2., 0., 1., 2., 0., 1., 2.],\n",
      "          [3., 4., 5., 3., 4., 5., 3., 4., 5.],\n",
      "          [6., 7., 8., 6., 7., 8., 6., 7., 8.]]]])\n",
      "------------------weights------------------\n",
      "tensor([[[[0.2500, 0.2500],\n",
      "          [0.2500, 0.2500]]]])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "------------------output------------------\n",
      "tensor([[[[2.0000, 3.0000, 2.5000, 2.0000, 3.0000, 2.5000, 2.0000, 3.0000],\n",
      "          [5.0000, 6.0000, 5.5000, 5.0000, 6.0000, 5.5000, 5.0000, 6.0000],\n",
      "          [3.5000, 4.5000, 4.0000, 3.5000, 4.5000, 4.0000, 3.5000, 4.5000],\n",
      "          [2.0000, 3.0000, 2.5000, 2.0000, 3.0000, 2.5000, 2.0000, 3.0000],\n",
      "          [5.0000, 6.0000, 5.5000, 5.0000, 6.0000, 5.5000, 5.0000, 6.0000],\n",
      "          [3.5000, 4.5000, 4.0000, 3.5000, 4.5000, 4.0000, 3.5000, 4.5000],\n",
      "          [2.0000, 3.0000, 2.5000, 2.0000, 3.0000, 2.5000, 2.0000, 3.0000],\n",
      "          [5.0000, 6.0000, 5.5000, 5.0000, 6.0000, 5.5000, 5.0000, 6.0000]]]])\n",
      "torch.Size([1, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "print(\"------------------input------------------\")\n",
    "im = torch.arange(9, dtype=torch.float).reshape((1, 1, 3, 3))\n",
    "print(im)\n",
    "print(im.shape)\n",
    "print(\"------------------padded input------------------\")\n",
    "# im_padded = torch.nn.ConstantPad2d((1, 1, 1, 1), value=10)(im)\n",
    "# im_padded = torch.nn.ReflectionPad2d((2, 2, 2, 2))(im)\n",
    "# im_padded = torch.nn.ReplicationPad2d((1, 1, 1, 1))(im)\n",
    "# im_padded = F.pad(im, pad=(2, 1, 3, 4), mode='constant', value=2)\n",
    "im_padded = F.pad(im, pad=(3, 3, 3, 3), mode='circular') # 'constant', 'reflect', 'replicate', 'circular'. Default: 'constant'\n",
    "print(im_padded)\n",
    "print(\"------------------weights------------------\")\n",
    "win_size = 2\n",
    "weights = torch.ones((win_size, win_size), dtype=torch.float)\n",
    "weights = weights/sum(sum(weights))\n",
    "weights= weights.reshape((1, 1, win_size, win_size))\n",
    "print(weights)\n",
    "print(weights.shape)\n",
    "print(\"------------------output------------------\")\n",
    "res = F.conv2d(im_padded, weight=weights, stride=1, padding=0) # default is zero padding\n",
    "print(res)\n",
    "print(res.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.nn.ConvTranspose2d\n",
    "+ `class torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride = 1, padding= 0, output_padding = 0, groups = 1, bias = True, dilation = 1, padding_mode = 'zeros')`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Theoretically understands this piece of code `x = torch.nn.ConvTranspose2d(in_channels, out_channels, kernel_size=f, stride =s, padding=p)(y)`\n",
    "\n",
    "Since\n",
    "\\begin{equation}\n",
    "w_y = \\lfloor\\frac{w_x+2p-f}{s}+1\\rfloor\n",
    "\\end{equation}\n",
    "then we have \n",
    "\\begin{eqnarray}\n",
    "w_x &=& \\lfloor(w_y-1)\\cdot s-2p+f\\rfloor\\\\\n",
    "&=& \\lfloor\\frac{w_y + (w_y-1)*(s-1) + 2(f-p-1) - f}{1} + 1\\rfloor\n",
    "\\end{eqnarray}\n",
    "which means\n",
    "+ Given an input image y with shape (width, height)=$(w_y, h_y)$.\n",
    "+ x = ConvTranspose2d(in_channels, out_channels, kernel_size=f, stride=s, padding=p)(y)\n",
    "+ step 1. 内部变换（与stride=s有关）：使用插值法，由y生成y_new，y_new.shape = $w_y + (w_y-1)*(s-1), h_y + (h_y-1)*(s-1)$\n",
    "+ step 2. 外部变换（与padding=p有关）：x = torch.nn.Conv2d(in_channels, out_channels, kernel_size=f, stride = 1, padding=f-p-1)(y_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------input------------------\n",
      "torch.Size([1, 16, 12, 12])\n",
      "------------------downsample------------------\n",
      "torch.Size([1, 16, 6, 6])\n",
      "------------------upsample------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 11, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"------------------input------------------\")\n",
    "input = torch.randn(1, 16, 12, 12)\n",
    "print(input.shape)\n",
    "print(\"------------------downsample------------------\")\n",
    "downsample = torch.nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "h = downsample(input)\n",
    "print(h.shape)\n",
    "print(\"------------------upsample------------------\")\n",
    "upsample = torch.nn.ConvTranspose2d(in_channels=16, out_channels=16, kernel_size=3, stride=2, padding=1)\n",
    "output = upsample(h, output_size=input.size())\n",
    "output = upsample(h)\n",
    "output.size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

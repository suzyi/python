{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transforms\n",
    "Guorui Shen, guorui233@outlook.com, Jul 30, 2020.\n",
    "+ `class torchvision.transforms.ToTensor`\n",
    "    + Convert a PIL Image or numpy.ndarray to tensor. Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n",
    "+ `class torchvision.transforms.Normalize(mean, std, inplace=False)`\n",
    "    + Normalize a tensor image with mean and standard deviation. Given mean: (mean[1],...,mean[n]) and std: (std[1],..,std[n]) for n channels, this transform will normalize each channel of the input torch.*Tensor i.e., output[channel] = (input[channel] - mean[channel]) / std[channel]\n",
    "+ `class torchvision.transforms.CenterCrop(size)`\n",
    "    + Crops the given image at the center. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions\n",
    "    + size (sequence or int) – Desired output size of the crop. If size is an int instead of sequence like (h, w), a square crop (size, size) is made. If provided a tuple or list of length 1, it will be interpreted as (size[0], size[0]).\n",
    "+ `torchvision.transforms.functional.center_crop(img: torch.Tensor, output_size: List[int])`\n",
    "    + Crops the given image at the center. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n",
    "+ `transforms.Resize(256)`\n",
    "    + input image is resized to be of size (256, 256)\n",
    "+ `transforms.RandomResizedCrop(224)`\n",
    "    + This will extract a patch of size (224, 224) from your input image randomly. So, it might pick this path from topleft, bottomright or anywhere in between. So, you are doing data augmentation in this part.\n",
    "+ `class torchvision.transforms.RandomCrop(size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant')`\n",
    "    + Crop the given image at a random location. The image can be a PIL Image or a Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n",
    "+ `class torchvision.transforms.RandomHorizontalFlip(p=0.5)`\n",
    "    + Horizontally flip the given image randomly with a given probability. The image can be a PIL Image or a torch Tensor, in which case it is expected to have […, H, W] shape, where … means an arbitrary number of leading dimensions.\n",
    "    + p (float) – probability of the image being flipped. Default value is 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `class torchvision.transforms.ToTensor` and `class torchvision.transforms.Normalize(mean, std, inplace=False)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform=transforms.Compose([\n",
    "    transforms.ToTensor(), # From int in the interval (0, 255) to float in [0, 1].\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # (x-mean)/variance. Images in mnist are allsingle channel.\n",
    "    ])\n",
    "dataset1 = datasets.MNIST('data', train=True, download=False,\n",
    "                   transform=transform)\n",
    "dataset2 = datasets.MNIST('data', train=False,\n",
    "                   transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(train_loader.dataset)= 60000 , len(test_loader.dataset)= 10000\n"
     ]
    }
   ],
   "source": [
    "kwargs = {'batch_size': 8}\n",
    "train_loader = torch.utils.data.DataLoader(dataset1,**kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(dataset2, **kwargs)\n",
    "print(\"len(train_loader.dataset)=\", len(train_loader.dataset), \", len(test_loader.dataset)=\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "<built-in method type of Tensor object at 0x000001F159730080>\n",
      "torch.Size([60000])\n",
      "<built-in method type of Tensor object at 0x000001F15972FAC0>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.data.shape)\n",
    "print(train_loader.dataset.data.type)\n",
    "print(train_loader.dataset.targets.shape)\n",
    "print(train_loader.dataset.targets.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "<built-in method type of Tensor object at 0x000001F159730080>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda3\\lib\\site-packages\\torchvision\\datasets\\mnist.py:55: UserWarning: train_data has been renamed data\n",
      "  warnings.warn(\"train_data has been renamed data\")\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.train_data.shape)\n",
    "print(train_loader.dataset.train_data.type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.1307,), std=(0.3081,))\n",
      "           )\n",
      "tensor([-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242])\n",
      "tensor([-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
      "        -0.4242, -0.4242, -0.4242, -0.4242])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset)\n",
    "# print(train_loader.dataset[0])\n",
    "# print(train_loader.dataset[0][0])\n",
    "print(train_loader.dataset[0][0][0][0])\n",
    "print(train_loader.dataset[0][0][0][27])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 28, 28])\n",
      "tensor([5, 0, 4, 1, 9, 2, 1, 3])\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "im = dataiter.next()\n",
    "print(im[0].shape)\n",
    "print(im[1])\n",
    "print(len(im[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `class torchvision.transforms.CenterCrop(size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im_cropped = transforms.functional.center_crop(img=im[0], output_size=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CenterCrop_size = 20\n",
    "transform=transforms.Compose([\n",
    "#     transforms.CenterCrop(size=CenterCrop_size), # This line should be placed before `transforms.ToTensor()`.\n",
    "    transforms.RandomCrop(size=CenterCrop_size, padding=None, pad_if_needed=False, fill=0, padding_mode='constant'),\n",
    "#     transforms.Resize(CenterCrop_size),\n",
    "#     transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ToTensor(), # From int in the interval (0, 255) to float in [0, 1].\n",
    "    ])\n",
    "dataset1_cropped = datasets.MNIST('data', train=True, download=False, transform=transform)\n",
    "train_loader_cropped = torch.utils.data.DataLoader(dataset1_cropped,**kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset1_cropped = datasets.ImageFolder(root='./ssim-AE-20200727/train-original-10imgs', transform=transform)\n",
    "# train_loader_cropped = torch.utils.data.DataLoader(dataset=dataset1_cropped, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    npimg_final = np.transpose(npimg, (1, 2, 0))\n",
    "    print(npimg_final.shape)\n",
    "    plt.imshow(npimg_final)\n",
    "    plt.show()\n",
    "    \n",
    "def imshow_mnist(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    print(npimg.shape)\n",
    "    npimg_final = npimg.reshape((CenterCrop_size, CenterCrop_size))\n",
    "    print(npimg_final.shape)\n",
    "    plt.imshow(npimg_final, cmap='Greys') # For single channel images\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both `enumerate` and `iter` can be used to display DataLoader\n",
    "### method 1- enumerate\n",
    "+ use this one if there is a few images to display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, data in enumerate(train_loader_cropped):\n",
    "#     inputs, labels = data\n",
    "#     print(\"i={}, inputs.shape={}, labels.shape={}\".format(i, inputs.shape, labels.shape))\n",
    "#     imshow(inputs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### method 2- iter\n",
    "+ use this one if there are many images to display, such as minist dataset, so that you don't have a lot of redundant images to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 1, 20, 20])\n",
      "(1, 20, 20)\n",
      "(20, 20)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader_cropped)\n",
    "im = dataiter.next()\n",
    "print(im[0].shape)\n",
    "imshow_mnist(im[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f503fe1b8c95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
